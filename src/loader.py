import os
import yaml
import torch
import json
from argparse import Namespace
from pix2tex.cli import LatexOCR

class RobustLatexOCR:
    def __init__(self, asset_path: str):
        print("ğŸ” Starting RobustLatexOCR Initialization (Corrected Final Mode)...")
        
        self.weights = os.path.join(asset_path, "weights.pth")
        self.resizer = os.path.join(asset_path, "resizer.pth")
        self.tokenizer_path = os.path.join(asset_path, "tokenizer.json")
        self.raw_config_path = os.path.join(asset_path, "settings.yaml")
        self.clean_config_path = os.path.join(asset_path, "clean_settings.yaml")
        
        # 1. å¿…é ˆã‚¢ã‚»ãƒƒãƒˆã®ç¢ºèª
        for p in [self.weights, self.resizer]:
            if not os.path.exists(p):
                raise RuntimeError(f"Critical Asset Missing: {p}")

        # 2. Tokenizerã‹ã‚‰num_tokensï¼ˆèªå½™æ•°ï¼‰ã‚’è‡ªå‹•å–å¾—
        # ã“ã‚ŒãŒãªã„ã¨ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®åˆæœŸåŒ–ã§æ­»ã«ã¾ã™
        vocab_size = 8000 # ä¸‡ãŒä¸€ã®ãŸã‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        if os.path.exists(self.tokenizer_path):
            try:
                with open(self.tokenizer_path, 'r', encoding='utf-8') as f:
                    tokenizer_data = json.load(f)
                    # tokenizer.jsonã®æ§‹é€ ã«åˆã‚ã›ã¦vocabã‚µã‚¤ã‚ºã‚’å–å¾—
                    if 'model' in tokenizer_data and 'vocab' in tokenizer_data['model']:
                        vocab_size = len(tokenizer_data['model']['vocab'])
                        print(f"ğŸ“Š Auto-detected vocab size (num_tokens): {vocab_size}")
            except Exception as e:
                print(f"âš ï¸ Failed to read tokenizer.json: {e}. Using default: {vocab_size}")
        else:
             print(f"âš ï¸ Tokenizer not found at {self.tokenizer_path}. Using default vocab size: {vocab_size}")

        # 3. ã€çœŸã®å®Œå…¨ç¶²ç¾…ã€‘å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å®šç¾©
        full_defaults = {
            # --- å¿…é ˆãƒ¢ãƒ‡ãƒ«æ§‹é€  ---
            'num_tokens': vocab_size, # ã€ä»Šå›è¿½åŠ ã€‘ã“ã‚ŒãŒæ¬ ã‘ã¦ã„ã¾ã—ãŸ
            'max_seq_len': 512,
            'dim': 256,
            'encoder_structure': 'hybrid',
            'decoder_structure': 'transformer',
            
            # --- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ ---
            'backbone_layers': [2, 3, 7],
            'encoder_depth': 4,
            'channels': 1,
            'patch_size': 16,
            
            # --- ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ ---
            'num_layers': 4,
            'heads': 8,
            'ff_dim': 1024,
            'dropout': 0.1,
            'emb_dropout': 0.1,
            
            # --- decoder_args (ãƒã‚¹ãƒˆç”¨: pix2texã®å®Ÿè£…ã«ã‚ˆã£ã¦ã¯ã“ã“ã‚’è¦‹ã‚‹) ---
            'decoder_args': {
                'max_seq_len': 512,
                'dim': 256,
                'num_layers': 4,
                'heads': 8,
                'dropout': 0.1,
                'num_tokens': vocab_size, # ã“ã“ã«ã‚‚å¿µã®ãŸã‚
                'ff_dim': 1024,
            },
            
            # --- ç”»åƒã‚µã‚¤ã‚º (intä¿è¨¼) ---
            'max_height': 192,
            'max_width': 672,
            'min_height': 32,
            'min_width': 32,
            
            # --- ãƒˆãƒ¼ã‚¯ãƒ³ID ---
            'pad_token': 0,
            'bos_token': 1,
            'eos_token': 2,
            'unk_token': 3,
            
            # --- å­¦ç¿’ãƒ»ã‚·ã‚¹ãƒ†ãƒ è¨­å®š ---
            'temperature': 0.2,
            'batchsize': 10,
            'micro_batchsize': -1,
            'optimizer': 'AdamW',
            'scheduler': 'OneCycleLR',
            'lr': 0.001,
            'min_lr': 0.0001,
            'weight_decay': 0.05,
            'seed': 42,
            'epochs': 10,
            'wandb': False,
            'device': 'cpu',
            'gpu_devices': [],
            'sample_freq': 2000,
            'val_freq': 1,
            'log_freq': 100,
            'workers': 1,
            
            # --- ã‚·ã‚¹ãƒ†ãƒ è¨­å®š ---
            'checkpoint': self.weights,
            'tokenizer': self.tokenizer_path,
            'id': None,
            'name': 'math_ocr_model',
            'no_cuda': True,
            'no_resize': False,
            'config': self.clean_config_path,
        }

        # 4. ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã®ãƒ­ãƒ¼ãƒ‰ (å‚è€ƒç¨‹åº¦)
        user_config = {}
        try:
            if os.path.exists(self.raw_config_path):
                with open(self.raw_config_path, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f) or {}
                    print("ğŸ“‚ User config loaded for reference.")
        except Exception:
            pass

        # 5. å®‰å…¨ãªãƒãƒ¼ã‚¸
        for k, v in user_config.items():
            # ãƒªã‚¹ãƒˆå‹ã®å¯¸æ³•æŒ‡å®šã¯å±•é–‹ã—ã¦å–ã‚Šè¾¼ã‚€
            if k == 'max_dimensions' and isinstance(v, list):
                full_defaults['max_height'] = int(v[0])
                full_defaults['max_width'] = int(v[1])
            elif k == 'min_dimensions' and isinstance(v, list):
                full_defaults['min_height'] = int(v[0])
                full_defaults['min_width'] = int(v[1])
            # åŸºæœ¬å‹ã®ã¿å–ã‚Šè¾¼ã‚€
            elif k in full_defaults and isinstance(v, (int, float, str, bool)):
                full_defaults[k] = v
            # decoder_argsã®ãƒãƒ¼ã‚¸
            elif k == 'decoder_args' and isinstance(v, dict):
                full_defaults['decoder_args'].update(v)

        # 6. ã‚¯ãƒªãƒ¼ãƒ³ãªè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
        try:
            with open(self.clean_config_path, 'w', encoding='utf-8') as f:
                yaml.dump(full_defaults, f)
            print(f"ğŸ”§ Generated robust config at: {self.clean_config_path}")
        except Exception as e:
            raise RuntimeError(f"Failed to write clean config: {e}")
        
        # 7. Namespaceç”Ÿæˆ
        args = Namespace(**full_defaults)
        
        print(f"ğŸš€ Initializing LatexOCR with:")
        print(f"   - num_tokens: {args.num_tokens}") # ç¢ºèªç”¨ãƒ­ã‚°
        print(f"   - encoder_depth: {args.encoder_depth}")
        print(f"   - dim: {args.dim}")
        
        try:
            self.engine = LatexOCR(args)
            if torch.cuda.is_available():
                self.engine.model.cuda()
            print("âœ… Model initialized successfully!")
        except Exception as e:
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Model Init Failed: {e}")

    def predict(self, image):
        try:
            return f"${self.engine(image)}$"
        except Exception as e:
            return f"\\text{{Error: {str(e)}}}"
