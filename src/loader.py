import os
import yaml
import torch
import json
from argparse import Namespace
from pix2tex.cli import LatexOCR

class RobustLatexOCR:
    def __init__(self, asset_path: str):
        print("ğŸ” Starting RobustLatexOCR Initialization (Deduplicated Final Mode)...")
        
        self.weights = os.path.join(asset_path, "weights.pth")
        self.resizer = os.path.join(asset_path, "resizer.pth")
        self.tokenizer_path = os.path.join(asset_path, "tokenizer.json")
        self.raw_config_path = os.path.join(asset_path, "settings.yaml")
        self.clean_config_path = os.path.join(asset_path, "clean_settings.yaml")
        
        # 1. å¿…é ˆã‚¢ã‚»ãƒƒãƒˆã®ç¢ºèª
        for p in [self.weights, self.resizer]:
            if not os.path.exists(p):
                raise RuntimeError(f"Critical Asset Missing: {p}")

        # 2. Tokenizerã‹ã‚‰num_tokensã‚’å–å¾—
        vocab_size = 8000
        if os.path.exists(self.tokenizer_path):
            try:
                with open(self.tokenizer_path, 'r', encoding='utf-8') as f:
                    tokenizer_data = json.load(f)
                    if 'model' in tokenizer_data and 'vocab' in tokenizer_data['model']:
                        vocab_size = len(tokenizer_data['model']['vocab'])
                        print(f"ğŸ“Š Auto-detected vocab size: {vocab_size}")
            except Exception:
                pass

        # 3. ã€ä¿®æ­£ç‚¹ã€‘ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å®šç¾©ï¼ˆé‡è¤‡æ’é™¤ï¼‰
        full_defaults = {
            # --- ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã“ã“ã§å€¤ã‚’æ±ºå®šï¼‰ ---
            'num_tokens': vocab_size,
            'max_seq_len': 512,
            'dim': 256,
            'encoder_structure': 'hybrid',
            'decoder_structure': 'transformer',
            
            'backbone_layers': [2, 3, 7],
            'encoder_depth': 4,
            'channels': 1,
            'patch_size': 16,
            
            'num_layers': 4,
            'heads': 8,
            'ff_dim': 1024,
            'dropout': 0.1,
            'emb_dropout': 0.1,
            
            # --- ã€é‡è¦ã€‘decoder_args ã‚’ç©ºã«ã™ã‚‹ ---
            # pix2texã¯ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã® dim ã‚„ heads ã‚’å¼•æ•°ã¨ã—ã¦ Decoder ã«æ¸¡ã—ã¾ã™ã€‚
            # ã“ã“ã«åŒã˜ã‚­ãƒ¼ï¼ˆdimç­‰ï¼‰ã‚’å…¥ã‚Œã‚‹ã¨ã€ŒäºŒé‡æ¸¡ã—ã€ã§ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ã¾ã™ã€‚
            # ç‹¬è‡ªã®è¨­å®šãŒå¿…è¦ãªå ´åˆä»¥å¤–ã¯ç©ºã«ã—ã¦ãŠãã®ãŒæ­£è§£ã§ã™ã€‚
            'decoder_args': {
                # 'dim': 256,      <-- å‰Šé™¤ (ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã¨é‡è¤‡ã™ã‚‹ãŸã‚)
                # 'num_layers': 4, <-- å‰Šé™¤
                # 'heads': 8,      <-- å‰Šé™¤
                # 'ff_dim': 1024,  <-- å‰Šé™¤
                'attn_on_attn': True, # å¿…è¦ã§ã‚ã‚Œã°å›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿æ®‹ã™
                'cross_attend': True,
                'ff_glu': True,
                'rel_pos_bias': False,
                'use_scalenorm': False,
            },
            
            # --- ç”»åƒã‚µã‚¤ã‚º ---
            'max_height': 192,
            'max_width': 672,
            'min_height': 32,
            'min_width': 32,
            
            # --- ãƒˆãƒ¼ã‚¯ãƒ³ID ---
            'pad_token': 0,
            'bos_token': 1,
            'eos_token': 2,
            'unk_token': 3,
            
            # --- ãã®ä»– ---
            'temperature': 0.2,
            'batchsize': 10,
            'micro_batchsize': -1,
            'optimizer': 'AdamW',
            'scheduler': 'OneCycleLR',
            'lr': 0.001,
            'min_lr': 0.0001,
            'weight_decay': 0.05,
            'seed': 42,
            'epochs': 10,
            'wandb': False,
            'device': 'cpu',
            'gpu_devices': [],
            'sample_freq': 2000,
            'val_freq': 1,
            'log_freq': 100,
            'workers': 1,
            
            'checkpoint': self.weights,
            'tokenizer': self.tokenizer_path,
            'id': None,
            'name': 'math_ocr_model',
            'no_cuda': True,
            'no_resize': False,
            'config': self.clean_config_path,
        }

        # 4. ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã®ãƒ­ãƒ¼ãƒ‰ (å‚è€ƒç¨‹åº¦)
        user_config = {}
        try:
            if os.path.exists(self.raw_config_path):
                with open(self.raw_config_path, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f) or {}
                    print("ğŸ“‚ User config loaded for reference.")
        except Exception:
            pass

        # 5. å®‰å…¨ãªãƒãƒ¼ã‚¸
        for k, v in user_config.items():
            if k == 'max_dimensions' and isinstance(v, list):
                full_defaults['max_height'] = int(v[0])
                full_defaults['max_width'] = int(v[1])
            elif k == 'min_dimensions' and isinstance(v, list):
                full_defaults['min_height'] = int(v[0])
                full_defaults['min_width'] = int(v[1])
            elif k in full_defaults and isinstance(v, (int, float, str, bool)):
                full_defaults[k] = v
            # decoder_argsã®ãƒãƒ¼ã‚¸ã¯æ…é‡ã«è¡Œã†ï¼ˆé‡è¤‡ã‚­ãƒ¼ã¯å…¥ã‚Œãªã„ï¼‰
            elif k == 'decoder_args' and isinstance(v, dict):
                for dk, dv in v.items():
                    # dim, heads, num_layers ãªã©ã¯ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã§åˆ¶å¾¡ã™ã‚‹ãŸã‚é™¤å¤–
                    if dk not in ['dim', 'heads', 'num_layers', 'ff_dim', 'num_tokens']:
                        full_defaults['decoder_args'][dk] = dv

        # 6. ã‚¯ãƒªãƒ¼ãƒ³ãªè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
        try:
            with open(self.clean_config_path, 'w', encoding='utf-8') as f:
                yaml.dump(full_defaults, f)
            print(f"ğŸ”§ Generated robust config at: {self.clean_config_path}")
        except Exception as e:
            raise RuntimeError(f"Failed to write clean config: {e}")
        
        # 7. Namespaceç”Ÿæˆ
        args = Namespace(**full_defaults)
        
        print(f"ğŸš€ Initializing LatexOCR with:")
        print(f"   - dim: {args.dim}")
        print(f"   - decoder_args keys: {list(args.decoder_args.keys())}") # é‡è¤‡ãŒãªã„ã‹ç¢ºèª
        
        try:
            self.engine = LatexOCR(args)
            if torch.cuda.is_available():
                self.engine.model.cuda()
            print("âœ… Model initialized successfully!")
        except Exception as e:
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Model Init Failed: {e}")

    def predict(self, image):
        try:
            return f"${self.engine(image)}$"
        except Exception as e:
            return f"\\text{{Error: {str(e)}}}"
